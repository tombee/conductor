# Cost Tracking

Monitor and control LLM costs across your workflows.

## Overview

Conductor tracks token usage and calculates costs for every LLM request. This enables:

- **Usage visibility** — See token counts per request, step, and workflow
- **Cost calculation** — Estimate spending based on provider pricing
- **Budget controls** — Set limits to prevent runaway costs
- **Alerting** — Get notified before budgets are exceeded

## Token Tracking

Every LLM request tracks token consumption:

| Token Type | Description |
|------------|-------------|
| **Prompt tokens** | Tokens in your input (prompt + system message) |
| **Completion tokens** | Tokens generated by the model |
| **Cache creation tokens** | Tokens written to prompt cache (Anthropic) |
| **Cache read tokens** | Tokens read from prompt cache (Anthropic) |

### Viewing Token Usage

**CLI output:**

```bash
$ conductor run workflow.yaml

[1/3] analyze... OK (1,245 tokens, $0.0037)
[2/3] summarize... OK (892 tokens, $0.0027)
[3/3] format... OK (156 tokens, $0.0005)

Total: 2,293 tokens, $0.0069
```

**Workflow output:**

Access token counts in subsequent steps:

```conductor
steps:
  - id: analyze
    type: llm
    prompt: "Analyze: {{.inputs.code}}"

  - id: log_usage
    shell.run:
      command: ["echo", "Used {{.steps.analyze.usage.total_tokens}} tokens"]
```

## Cost Calculation

Conductor calculates costs based on provider pricing:

```
cost = (prompt_tokens × input_rate) + (completion_tokens × output_rate)
```

!!! note "Pricing accuracy"
    Conductor uses published pricing from LLM providers. Actual costs may vary based on your account, volume discounts, or pricing changes. Always verify with your provider's billing dashboard.

    - [Anthropic Pricing](https://www.anthropic.com/pricing)
    - [OpenAI Pricing](https://openai.com/pricing)
    - [Google AI Pricing](https://cloud.google.com/vertex-ai/pricing)

### Cache Token Savings

With prompt caching (Anthropic), cached tokens cost less:

```
cache_savings = cache_read_tokens × (input_rate - cache_rate)
```

Conductor reports both actual cost and potential savings from caching.

## Budget Configuration

Set spending limits in your configuration:

```yaml
# ~/.config/conductor/config.yaml
cost:
  budget:
    daily_limit: 10.00    # USD per day
    monthly_limit: 200.00 # USD per month

  alerts:
    threshold: 0.80       # Alert at 80% of limit

  enforcement: warn       # warn | block
```

### Enforcement Modes

| Mode | Behavior |
|------|----------|
| `warn` | Log warning when budget exceeded, continue execution |
| `block` | Fail workflow when budget would be exceeded |

### Budget Scope

Budgets apply at the Conductor instance level. For multi-tenant deployments, configure separate Conductor instances or use the controller API for per-workflow limits.

## Prometheus Metrics

When running in controller mode, Conductor exports Prometheus metrics:

### Token Metrics

```promql
# Total tokens by provider and model
conductor_llm_tokens_total{provider="anthropic", model="claude-3-5-sonnet", type="prompt"}
conductor_llm_tokens_total{provider="anthropic", model="claude-3-5-sonnet", type="completion"}
conductor_llm_tokens_total{provider="anthropic", model="claude-3-5-sonnet", type="cache_read"}
conductor_llm_tokens_total{provider="anthropic", model="claude-3-5-sonnet", type="cache_creation"}
```

### Cost Metrics

```promql
# Cumulative cost in USD
conductor_llm_cost_usd{provider="anthropic"}

# Cost by workflow
conductor_workflow_cost_usd{workflow="code-review"}
```

### Example Queries

**Daily spend by provider:**

```promql
sum by (provider) (
  increase(conductor_llm_cost_usd[24h])
)
```

**Most expensive workflows:**

```promql
topk(10,
  sum by (workflow) (
    increase(conductor_workflow_cost_usd[7d])
  )
)
```

**Token efficiency (completion vs prompt ratio):**

```promql
sum(rate(conductor_llm_tokens_total{type="completion"}[1h]))
/
sum(rate(conductor_llm_tokens_total{type="prompt"}[1h]))
```

## Cost Optimization

### Use Appropriate Model Tiers

Match model capability to task complexity:

| Task | Tier | Cost Impact |
|------|------|-------------|
| Classification, extraction | `fast` | Lowest |
| Code review, summarization | `balanced` | Moderate |
| Architecture, research | `strategic` | Highest |

See [Model Tiers](../guides/model-tiers.md) for guidance.

### Enable Prompt Caching

For workflows with repeated system prompts (Anthropic):

```conductor
steps:
  - id: analyze
    type: llm
    model: balanced
    cache_control: true  # Enable prompt caching
    system: |
      You are a code reviewer...  # This gets cached
    prompt: "Review: {{.inputs.code}}"
```

### Minimize Token Usage

1. **Be concise** — Shorter prompts cost less
2. **Filter inputs** — Only include relevant context
3. **Use structured output** — Request specific formats
4. **Batch when possible** — Combine related requests

### Use Dry Run for Testing

Validate workflows without LLM costs:

```bash
conductor run workflow.yaml --dry-run
```

## Security Considerations

### Access Control

Cost data may reveal usage patterns. Restrict access to:

- Prometheus metrics endpoint
- Configuration files with budget settings
- Log files containing token counts

### Data Retention

Configure retention for cost records:

```yaml
cost:
  retention:
    days: 90  # Keep cost records for 90 days
```

### Audit Logging

Cost-related events are logged for audit:

- Budget threshold warnings
- Budget exceeded blocks
- Configuration changes

## Runbook: Investigating Cost Spikes

When costs increase unexpectedly:

### Step 1: Identify the Time Range

Check when the spike occurred:

```promql
# Cost rate over last 7 days
rate(conductor_llm_cost_usd[1h])
```

### Step 2: Find the Workflow

Identify which workflow(s) caused the increase:

```promql
topk(5,
  sum by (workflow) (
    increase(conductor_workflow_cost_usd{...}[24h])
  )
)
```

### Step 3: Analyze Token Patterns

Check if prompt or completion tokens increased:

```promql
# Prompt vs completion breakdown
sum by (type) (
  increase(conductor_llm_tokens_total{workflow="suspect-workflow"}[24h])
)
```

### Step 4: Review Recent Changes

- Check workflow file changes in version control
- Review input data changes
- Check for loops or foreach with unexpected counts

### Common Causes

| Symptom | Likely Cause | Fix |
|---------|--------------|-----|
| High prompt tokens | Large input data | Filter/truncate inputs |
| High completion tokens | Verbose output | Add output constraints |
| Many requests | Foreach with large array | Batch or limit iterations |
| Strategic tier overuse | Wrong tier selection | Downgrade to balanced/fast |

## See Also

- [Model Tiers](../guides/model-tiers.md) — Cost-effective tier selection
- [Monitoring](monitoring.md) — Observability setup
- [LLM Providers](../architecture/llm-providers.md) — Provider configuration
